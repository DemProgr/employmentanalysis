"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ ML –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Ç—Ä—É–¥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import (train_test_split, cross_val_score, 
                                   StratifiedKFold, RandomizedSearchCV,
                                   cross_validate)
from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,
                            HistGradientBoostingClassifier, VotingClassifier,
                            StackingClassifier)
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, roc_auc_score, classification_report,
                           confusion_matrix, precision_recall_curve, average_precision_score)
from sklearn.calibration import CalibratedClassifierCV
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.base import BaseEstimator, ClassifierMixin
import joblib
import logging
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

class AdvancedEmploymentClassifier(BaseEstimator, ClassifierMixin):
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Ç—Ä—É–¥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
    
    def __init__(self, model_type='xgboost', random_state=42):
        self.model_type = model_type
        self.random_state = random_state
        self.model = None
        self.base_model = None  # üî• –î–û–ë–ê–í–õ–ï–ù–û: —Ö—Ä–∞–Ω–∏–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –æ—Ç–¥–µ–ª—å–Ω–æ
        self.is_calibrated = False
        self.feature_importance_ = None
        self.classes_ = None
        
    def _get_base_model(self, model_type):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –ø–æ —Ç–∏–ø—É"""
        models = {
            'xgboost': XGBClassifier(
                random_state=self.random_state,
                eval_metric='logloss',
            ),
            'lightgbm': LGBMClassifier(
                random_state=self.random_state,
                verbose=-1
            ),
            'random_forest': RandomForestClassifier(
                random_state=self.random_state
            ),
            'gradient_boosting': GradientBoostingClassifier(
                random_state=self.random_state
            ),
            'logistic': LogisticRegression(
                random_state=self.random_state,
                max_iter=1000
            )
        }
        return models.get(model_type, models['xgboost'])
    
    def _get_param_distribution(self, model_type):
        """–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è RandomizedSearchCV"""
        param_distributions = {
            'xgboost': {
                'n_estimators': [100, 200, 300, 500],
                'max_depth': [3, 5, 7, 9],
                'learning_rate': [0.01, 0.05, 0.1, 0.2],
                'subsample': [0.8, 0.9, 1.0],
                'colsample_bytree': [0.8, 0.9, 1.0],
                'reg_alpha': [0, 0.1, 0.5, 1],
                'reg_lambda': [0, 0.1, 0.5, 1]
            },
            'lightgbm': {
                'n_estimators': [100, 200, 300, 500],
                'max_depth': [3, 5, 7, 9, -1],
                'learning_rate': [0.01, 0.05, 0.1, 0.2],
                'num_leaves': [31, 63, 127, 255],
                'subsample': [0.8, 0.9, 1.0],
                'colsample_bytree': [0.8, 0.9, 1.0],
                'reg_alpha': [0, 0.1, 0.5, 1],
                'reg_lambda': [0, 0.1, 0.5, 1]
            },
            'random_forest': {
                'n_estimators': [100, 200, 300, 500],
                'max_depth': [5, 10, 15, 20, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4],
                'max_features': ['sqrt', 'log2', None],
                'bootstrap': [True, False]
            },
            'gradient_boosting': {
                'n_estimators': [100, 200, 300],
                'learning_rate': [0.01, 0.05, 0.1, 0.2],
                'max_depth': [3, 5, 7, 9],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4],
                'subsample': [0.8, 0.9, 1.0]
            }
        }
        return param_distributions.get(model_type, {})
    
    def fit(self, X, y, optimize_hyperparams=True, cv_folds=5):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        try:
            logger.info(f"üéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ {self.model_type}...")
            
            base_model = self._get_base_model(self.model_type)
            self.classes_ = np.unique(y)
            
            if optimize_hyperparams and self._get_param_distribution(self.model_type):
                # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                param_dist = self._get_param_distribution(self.model_type)
                
                search = RandomizedSearchCV(
                    base_model,
                    param_dist,
                    n_iter=20,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                    cv=StratifiedKFold(n_splits=min(cv_folds, 3), shuffle=True, random_state=self.random_state),
                    scoring='roc_auc',
                    random_state=self.random_state,
                    n_jobs=-1,
                    verbose=0
                )
                
                search.fit(X, y)
                self.base_model = search.best_estimator_  # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
                logger.info(f"‚úÖ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {search.best_params_}")
                logger.info(f"‚úÖ –õ—É—á—à–∏–π ROC-AUC: {search.best_score_:.4f}")
                
            else:
                # –ü—Ä–æ—Å—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ
                self.base_model = base_model
                self.base_model.fit(X, y)
            
            # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –î–û –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏
            if hasattr(self.base_model, 'feature_importances_'):
                self.feature_importance_ = self.base_model.feature_importances_
            elif hasattr(self.base_model, 'coef_'):
                self.feature_importance_ = np.abs(self.base_model.coef_[0])
            
            # –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            self.model = CalibratedClassifierCV(
                self.base_model,  # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
                cv=min(3, cv_folds),
                method='isotonic'
            )
            self.model.fit(X, y)
            self.is_calibrated = True
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {self.model_type} —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –æ—Ç–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–∞")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
            raise
    
    def predict(self, X):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤"""
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
        return self.model.predict(X)
    
    def predict_proba(self, X):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π"""
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
        return self.model.predict_proba(X)
    
    def predict_employment_probability(self, X):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ç—Ä—É–¥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
        probabilities = self.predict_proba(X)
        return probabilities[:, 1]  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞ 1 (—Ç—Ä—É–¥–æ—É—Å—Ç—Ä–æ–µ–Ω)

class EnsembleEmploymentPredictor:
    """–ê–Ω—Å–∞–º–±–ª–µ–≤—ã–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏"""
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.models = {}
        self.ensemble_model = None
        self.feature_names = []
        self.is_trained = False
        
    def create_ensemble(self, X, y, feature_names):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π"""
        self.feature_names = feature_names
        
        # –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
        base_models = [
            ('xgboost', AdvancedEmploymentClassifier('xgboost', self.random_state)),
            ('lightgbm', AdvancedEmploymentClassifier('lightgbm', self.random_state)),
            ('random_forest', AdvancedEmploymentClassifier('random_forest', self.random_state))
        ]
        
        # –û–±—É—á–∞–µ–º –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
        for name, model in base_models:
            logger.info(f"üîß –û–±—É—á–µ–Ω–∏–µ {name}...")
            model.fit(X, y, optimize_hyperparams=True, cv_folds=3)
            self.models[name] = model
        
        # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—Ç–µ–∫–∏–Ω–≥–∞
        meta_model = LogisticRegression(random_state=self.random_state, max_iter=1000)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç–µ–∫–∏–Ω–≥–∞ —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
        self.ensemble_model = StackingClassifier(
            estimators=[(name, model.base_model) for name, model in base_models],  # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º base_model
            final_estimator=meta_model,
            cv=3,
            passthrough=False,
            n_jobs=-1
        )
        
        # –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è
        logger.info("üèóÔ∏è –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏...")
        self.ensemble_model.fit(X, y)
        
        self.is_trained = True
        logger.info("‚úÖ –ê–Ω—Å–∞–º–±–ª–µ–≤–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞")
    
    def predict_proba(self, X):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –∞–Ω—Å–∞–º–±–ª–µ–º"""
        if not self.is_trained:
            raise ValueError("–ê–Ω—Å–∞–º–±–ª—å –Ω–µ –æ–±—É—á–µ–Ω")
        return self.ensemble_model.predict_proba(X)
    
    def predict_employment_probability(self, X):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ç—Ä—É–¥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
        probabilities = self.predict_proba(X)
        return probabilities[:, 1]
    
    def get_model_weights(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–µ–π –≤ –∞–Ω—Å–∞–º–±–ª–µ"""
        if hasattr(self.ensemble_model.final_estimator_, 'coef_'):
            return self.ensemble_model.final_estimator_.coef_[0]
        return None

class AdvancedEmploymentPredictor:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å —Ç—Ä—É–¥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
    
    def __init__(self, use_ensemble=True, random_state=42):
        self.use_ensemble = use_ensemble
        self.random_state = random_state
        self.feature_engineer = None
        self.model = None
        self.ensemble_predictor = None
        self.performance_metrics = {}
        self.is_trained = False
        
    def train(self, df, target_column='employed', test_size=0.2):
        """–û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –º–æ–¥–µ–ª–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            # –ò–º–ø–æ—Ä—Ç –∑–¥–µ—Å—å —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏—Ö –∏–º–ø–æ—Ä—Ç–æ–≤
            from advanced_feature_engineer import AdvancedFeatureEngineer
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è feature engineer
            self.feature_engineer = AdvancedFeatureEngineer()
            
            # üî• –ü–†–û–í–ï–†–ö–ê –î–ê–ù–ù–´–• –ü–ï–†–ï–î –û–ë–£–ß–ï–ù–ò–ï–ú
            required_columns = ['gpa', 'internships', 'projects', 'certificates', 
                            'graduation_year', 'salary_byn', 'job_search_duration',
                            'faculty', 'university', 'location', 'employed']
            
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                logger.warning(f"‚ö†Ô∏è –í –¥–∞–Ω–Ω—ã—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
                # –ü–æ–ø—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –±–∞–∑–æ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                for col in missing_columns:
                    if col == 'salary_byn':
                        df[col] = df.get('salary', 0)
                    elif col == 'job_search_duration':
                        df[col] = 90  # –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                    elif col == 'location':
                        df[col] = '–ú–∏–Ω—Å–∫'  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    else:
                        df[col] = 0
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X_processed, y, feature_names = self.feature_engineer.prepare_features(
                df, target_column, fit=True
            )
            
            if len(X_processed) < 50:
                logger.warning("‚ö†Ô∏è –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è")
                return False
            
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
            from sklearn.model_selection import train_test_split
            X_train, X_test, y_train, y_test = train_test_split(
                X_processed, y, 
                test_size=test_size, 
                random_state=self.random_state, 
                stratify=y
            )
            
            # üî• –ó–ê–ü–ò–°–´–í–ê–ï–ú –ò–ù–§–û–†–ú–ê–¶–ò–Æ –û –ü–†–ò–ó–ù–ê–ö–ê–•
            logger.info(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {len(feature_names)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            logger.info(f"üìã –ü—Ä–∏–∑–Ω–∞–∫–∏: {feature_names[:10]}...")  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
            
            if self.use_ensemble:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–Ω—Å–∞–º–±–ª—å
                self.ensemble_predictor = EnsembleEmploymentPredictor(self.random_state)
                self.ensemble_predictor.create_ensemble(X_train, y_train, feature_names)
                
                # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                y_pred_proba = self.ensemble_predictor.predict_employment_probability(X_test)
                self._evaluate_model(y_test, y_pred_proba, "Ensemble")
                
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª—É—á—à—É—é –æ–¥–∏–Ω–æ—á–Ω—É—é –º–æ–¥–µ–ª—å
                best_model = AdvancedEmploymentClassifier('xgboost', self.random_state)
                best_model.fit(X_train, y_train, optimize_hyperparams=True, cv_folds=3)
                self.model = best_model
                
                # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                y_pred_proba = self.model.predict_employment_probability(X_test)
                self._evaluate_model(y_test, y_pred_proba, "XGBoost")
            
            self.is_trained = True
            logger.info("üéâ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
            
            # üî• –°–û–•–†–ê–ù–Ø–ï–ú –ò–ù–§–û–†–ú–ê–¶–ò–Æ –û –ü–†–ò–ó–ù–ê–ö–ê–•
            self.feature_names = feature_names
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False
    
    def _evaluate_model(self, y_true, y_pred_proba, model_name):
        """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏"""
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
            y_pred = (y_pred_proba > 0.5).astype(int)
            
            metrics = {
                'roc_auc': roc_auc_score(y_true, y_pred_proba),
                'accuracy': accuracy_score(y_true, y_pred),
                'precision': precision_score(y_true, y_pred, zero_division=0),
                'recall': recall_score(y_true, y_pred, zero_division=0),
                'f1': f1_score(y_true, y_pred, zero_division=0)
            }
            
            self.performance_metrics[model_name] = metrics
            
            logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã {model_name}:")
            for metric, value in metrics.items():
                logger.info(f"   {metric}: {value:.4f}")
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
    
    def predict(self, student_data):
        """–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ç—Ä—É–¥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
        if not self.is_trained:
            logger.error("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
            return None
        
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X_processed, _, _ = self.feature_engineer.prepare_features(
                student_data, fit=False
            )
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            if self.use_ensemble and self.ensemble_predictor:
                probability = self.ensemble_predictor.predict_employment_probability(X_processed)
            elif self.model:
                probability = self.model.predict_employment_probability(X_processed)
            else:
                logger.error("‚ùå –ù–µ—Ç –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏")
                return None
            
            return probability[0] if len(probability) == 1 else probability
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            return None
    
    def get_feature_importance(self, top_n=15):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        if not self.is_trained:
            return None
        
        try:
            feature_names = self.feature_engineer.get_feature_names()
            
            if self.model and hasattr(self.model, 'feature_importance_'):
                importances = self.model.feature_importance_
            elif self.ensemble_predictor:
                # –î–ª—è –∞–Ω—Å–∞–º–±–ª—è –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–¥–Ω—é—é –≤–∞–∂–Ω–æ—Å—Ç—å –∏–∑ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
                importances = np.zeros(len(feature_names))
                count = 0
                for name, model in self.ensemble_predictor.models.items():
                    if hasattr(model, 'feature_importance_') and model.feature_importance_ is not None:
                        importances += model.feature_importance_
                        count += 1
                if count > 0:
                    importances /= count
            else:
                return None
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
            indices = np.argsort(importances)[::-1]
            
            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ top_n –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            top_features = []
            for i in indices[:min(top_n, len(importances))]:
                if i < len(feature_names):
                    top_features.append((feature_names[i], importances[i]))
            
            return top_features
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
            return None
    
    def save_model(self, filepath):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        try:
            model_data = {
                'feature_engineer': self.feature_engineer,
                'model': self.model,
                'ensemble_predictor': self.ensemble_predictor,
                'performance_metrics': self.performance_metrics,
                'is_trained': self.is_trained
            }
            joblib.dump(model_data, filepath)
            logger.info(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {filepath}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
    
    def load_model(self, filepath):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        try:
            model_data = joblib.load(filepath)
            self.feature_engineer = model_data['feature_engineer']
            self.model = model_data['model']
            self.ensemble_predictor = model_data['ensemble_predictor']
            self.performance_metrics = model_data['performance_metrics']
            self.is_trained = model_data['is_trained']
            logger.info(f"üìÇ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {filepath}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")